{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#python 2/3 compatibility\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "#packages\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#set graphs to plot within env\n",
    "%matplotlib inline\n",
    "#set style of graphs\n",
    "sns.set(rc={'figure.figsize':(12, 6)})\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add code to input data\n",
    "#for now, uploads data file from local drive\n",
    "dataset =  pd.read_csv('mergeCleCinSave.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view dist + corr of all features, uncomment to run\n",
    "#sns.pairplot(dataset[['SF', 'Floors','Year Built', 'Value', 'E annual', 'G annual', 'Estimated Savings']], diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the target feature gas specifically to ID outliers and dist.\n",
    "#uncommet to run\n",
    "#fig, (boxplot, histogram) = plt.subplots(2, sharex=True, figsize=(16, 8), gridspec_kw={\"height_ratios\": (.15, .85)})\n",
    "#sns.boxplot(dataset['Estimated Savings'], ax=boxplot)\n",
    "#sns.distplot(dataset['Estimated Savings'], ax=histogram)\n",
    "#boxplot.set(xlabel='')\n",
    "#plt.title('Distribution of \"Annual Estimated Savings\"', fontsize=24)\n",
    "#plt.xlabel('');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train/test sets 80%/20%\n",
    "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainPredSaveScaler.save']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creates training data stats \n",
    "train_stats = train_dataset.describe()\n",
    "train_stats.pop(\"Estimated Savings\")\n",
    "train_stats = train_stats.transpose()\n",
    "\n",
    "#save train stats for norm. function\n",
    "scaler_filename = \"trainPredSaveScaler.save\"\n",
    "joblib.dump(train_stats, scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates train and test datasets target variable: Estimated Savings dollar value\n",
    "#and removes target variable from feature variable dataset\n",
    "train_labels = train_dataset.pop('Estimated Savings')\n",
    "test_labels = test_dataset.pop('Estimated Savings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to normalize train and test datasets to dist. range from train data\n",
    "def norm(x):\n",
    "    return (x - train_stats['mean']) / train_stats['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized train and test datasets\n",
    "normed_train_data = norm(train_dataset)\n",
    "normed_test_data = norm(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view first row of normalized dataset\n",
    "#uncomment to run\n",
    "#normed_test_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---DEEP LEARNING MODEL FRAMEWORK---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Learning model, 4 layers deep, 2 fully connected layers\n",
    "#can adjust optimizer by commenting/uncommeting 'optimizer' variable\n",
    "#loss func. set to mod. penalize model for large errors\n",
    "\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n",
    "    layers.Dropout(0.5), #50% data random dropout to reduce overfitting training data\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5), #50% data random dropout to reduce overfitting training data\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "  #optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "  model.compile(loss='mean_squared_logarithmic_error',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calls ML model and assigns it to the variable 'model'\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---TESTING/FINE-TUNING USE CODE BELOW---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs the above model for a set number of epochs\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "history = model.fit(\n",
    "    normed_train_data, train_labels,\n",
    "    epochs=EPOCHS, validation_split = 0.2, verbose=0, \n",
    "    callbacks=[tfdocs.modeling.EpochDots()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows last 5 epochs metrics of loss\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfdocs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7d6d181c6d58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#set plot with a smoothing parameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplotter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfdocs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHistoryPlotter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmoothing_std\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tfdocs' is not defined"
     ]
    }
   ],
   "source": [
    "#set plot with a smoothing parameter\n",
    "plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots Mean Absolute Error\n",
    "plotter.plot({'Basic': history}, metric = \"mae\")\n",
    "plt.ylabel('MAE [Est. Gas Usage]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots Mean Squared Error\n",
    "plotter.plot({'Basic': history}, metric = \"mse\")\n",
    "plt.ylabel('MSE [Est. Savings^2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test/train with an early stopping parameter to prevent overfitting\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "early_history = model.fit(normed_train_data, train_labels, \n",
    "                    epochs=EPOCHS, validation_split = 0.2, verbose=0, \n",
    "                    callbacks=[early_stop, tfdocs.modeling.EpochDots()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots Mean Absolute Error for the early stopping model\n",
    "plotter.plot({'Early Stopping': early_history}, metric = \"mae\")\n",
    "plt.ylabel('MAE [Est. Savings]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots predictions versus truth values\n",
    "test_predictions = model.predict(normed_test_data).flatten()\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('True Values [Est. Gas]')\n",
    "plt.ylabel('Predictions [Est. Gas]')\n",
    "lims = [0, 2000]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist. of errors by number of errors and diff. between predict and actual value\n",
    "error = test_predictions - test_labels\n",
    "plt.hist(error, bins = 20)\n",
    "plt.xlabel(\"Prediction Error [Est. Savings]\")\n",
    "_ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints the metrics for the best epoch of the trained model\n",
    "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)\n",
    "print(\"Testing set Mean Abs Error: {:5.2f} Est. Gas Usage\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---TO TRAIN AND SAVE A MODEL RUN BELOW---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13140cdefd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=200)\n",
    "\n",
    "model.fit(normed_train_data, train_labels, epochs=1000, \n",
    "          validation_split = 0.2, verbose=0, \n",
    "          callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots predictions versus truth values\n",
    "test_predictions = model.predict(normed_test_data).flatten()\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('True Values [Est. Savings]')\n",
    "plt.ylabel('Predictions [Est. Savings]')\n",
    "lims = [0, 2000]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist. of errors by number of errors and diff. between predict and actual value\n",
    "error = test_predictions - test_labels\n",
    "plt.hist(error, bins = 20)\n",
    "plt.xlabel(\"Prediction Error [Est. Savings]\")\n",
    "_ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 - 0s - loss: 0.2049 - mae: 192.3001 - mse: 58961.9141\n",
      "Testing set Mean Abs Error: 192.30 \n"
     ]
    }
   ],
   "source": [
    "#check models loss, mean actual error rate, mean squared error rate\n",
    "#uncomment to run\n",
    "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)\n",
    "print(\"Testing set Mean Abs Error: {:5.2f} \".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves TF model weights as name noted below\n",
    "model.save('predSaveDollar.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model, serialized model to JSON format\n",
    "model_json = model.to_json()\n",
    "with open(\"predSaveModel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
